{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11f92f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "#https://www.nltk.org/\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#import tensorflow as tf\n",
    "import ast\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#Decision trees for regression\n",
    "#MSE for loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cac52835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keybert import KeyBERT\n",
    "# kw_model = KeyBERT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f93e686d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>synopsis</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Haikyuu!! Second Season</td>\n",
       "      <td>Following their participation at the Inter-Hig...</td>\n",
       "      <td>8.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shigatsu wa Kimi no Uso</td>\n",
       "      <td>Music accompanies the path of the human metron...</td>\n",
       "      <td>8.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Made in Abyss</td>\n",
       "      <td>The Abyss—a gaping chasm stretching down into ...</td>\n",
       "      <td>8.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fullmetal Alchemist: Brotherhood</td>\n",
       "      <td>\"In order for something to be obtained, someth...</td>\n",
       "      <td>9.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kizumonogatari III: Reiketsu-hen</td>\n",
       "      <td>After helping revive the legendary vampire Kis...</td>\n",
       "      <td>8.83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              title  \\\n",
       "0           Haikyuu!! Second Season   \n",
       "1           Shigatsu wa Kimi no Uso   \n",
       "2                     Made in Abyss   \n",
       "3  Fullmetal Alchemist: Brotherhood   \n",
       "4  Kizumonogatari III: Reiketsu-hen   \n",
       "\n",
       "                                            synopsis  score  \n",
       "0  Following their participation at the Inter-Hig...   8.82  \n",
       "1  Music accompanies the path of the human metron...   8.83  \n",
       "2  The Abyss—a gaping chasm stretching down into ...   8.83  \n",
       "3  \"In order for something to be obtained, someth...   9.23  \n",
       "4  After helping revive the legendary vampire Kis...   8.83  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extract the dataset we are interested in, synopsis, name, and rating.\n",
    "df = pd.read_csv('DataSet/animes.csv')\n",
    "df = df[['title','synopsis','score']]\n",
    "df.dropna(inplace=True)\n",
    "df = df[~df.title.duplicated(keep='first')]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "353283b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15192\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(15192, 100)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Method one for generating X vectors: hash.\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "X = df['synopsis'].to_numpy()\n",
    "y = df['score'].to_numpy()\n",
    "y = np.rint(y)\n",
    "print(len(y))\n",
    "vectorizer = HashingVectorizer(n_features=100, norm = None, alternate_sign = False)\n",
    "hashed_X = vectorizer.fit_transform(X)\n",
    "#print(hashed_X)\n",
    "hashed_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca612f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building training sets and test sets.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(hashed_X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc976f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.33991444554129646\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "model = DecisionTreeClassifier(max_depth=100)\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "score = accuracy_score(y_test, pred)\n",
    "print(\"Score: \" + str(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64b2d82",
   "metadata": {},
   "source": [
    "Accuracy of about 35%, which is better than guessing randomly (10%), but not very good for practical predictions.\n",
    "\n",
    "Pros of this method: Easy to implement\n",
    "\n",
    "Cons: Not interpretable, I have no idea what type of synopsis correspond to the predictions made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "373cdc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "#https://www.nltk.org/book/ch06.html\n",
    "#https://www.datacamp.com/tutorial/text-analytics-beginners-nltk#sentiment-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83423df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "#tokenizer to remove unwanted elements from out data like symbols and numbers\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv = CountVectorizer(lowercase=True,stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize)\n",
    "text_counts= cv.fit_transform(df['synopsis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67b49d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://splunktool.com/unknown-label-type-error-when-sklearn-naive-bayes-used-with-floating-point-numbers\n",
    "score_int = df['score'].to_numpy()\n",
    "score_int = np.rint(score_int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_counts, score_int, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d6e4df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.48244844229925404\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "# Model Generation Using Multinomial Naive Bayes\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "predicted= clf.predict(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, predicted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70ed145",
   "metadata": {},
   "source": [
    "The accuracy is slightly higher with nltk implemented at around 47%, but there are still improvements to be made. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3dd8be29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "#Using old tokenizer, subject to change.\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv = CountVectorizer(lowercase=True,stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize)\n",
    "X= cv.fit_transform(df['synopsis'])\n",
    "\n",
    "#Now that we are using decision tree regression, we can use the scores as floats.\n",
    "y = df['score'].to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "\n",
    "tree_reg.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred = tree_reg.predict(X_test)\n",
    "\n",
    "y_train_pred = tree_reg.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a075499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.023303050041746\n",
      "0.0027221966491129085\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "print(mean_squared_error(y_test, y_test_pred))\n",
    "print(mean_squared_error(y_train, y_train_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d277d4",
   "metadata": {},
   "source": [
    "It seems this initial run on regression tree overfit the data, it has a really low mean squared error on the training data, but seems to be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87ca3962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9775230180832845\n",
      "0.002713765829728543\n"
     ]
    }
   ],
   "source": [
    "#https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "#with stop words & max_df 0.7 & ngram_range = (1,2)\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv = CountVectorizer(lowercase=True,stop_words='english', max_df = 0.7, ngram_range = (1,2), tokenizer = token.tokenize)\n",
    "X= cv.fit_transform(df['synopsis'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "tree_reg.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred = tree_reg.predict(X_test)\n",
    "\n",
    "y_train_pred = tree_reg.predict(X_train)\n",
    "\n",
    "print(mean_squared_error(y_test, y_test_pred))\n",
    "print(mean_squared_error(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2847afe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0332234147804837\n",
      "0.002550271377970033\n"
     ]
    }
   ],
   "source": [
    "#without stop words & ngram_range = (1,1)\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv = CountVectorizer(lowercase=True, ngram_range = (1,1), tokenizer = token.tokenize)\n",
    "X= cv.fit_transform(df['synopsis'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "tree_reg.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred = tree_reg.predict(X_test)\n",
    "\n",
    "y_train_pred = tree_reg.predict(X_train)\n",
    "\n",
    "print(mean_squared_error(y_test, y_test_pred))\n",
    "print(mean_squared_error(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61716f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0325918269175687\n",
      "0.002550271377970033\n"
     ]
    }
   ],
   "source": [
    "#without stop words & ngram_range = (1,2)\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv = CountVectorizer(lowercase=True, ngram_range = (1,2), tokenizer = token.tokenize)\n",
    "X= cv.fit_transform(df['synopsis'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "tree_reg.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred = tree_reg.predict(X_test)\n",
    "\n",
    "y_train_pred = tree_reg.predict(X_train)\n",
    "\n",
    "print(mean_squared_error(y_test, y_test_pred))\n",
    "print(mean_squared_error(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6f0209",
   "metadata": {},
   "source": [
    "The removal of stop words helps get a better score. Changing the ngram to unigrams and bigrams also helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed2c1497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.977287710809139\n",
      "0.002713765829728543\n"
     ]
    }
   ],
   "source": [
    "#with stop words & max_df 0.7 & ngram_range = (1,2)\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv = CountVectorizer(lowercase=True,stop_words='english', max_df = 0.7, ngram_range = (1,2), tokenizer = token.tokenize)\n",
    "X= cv.fit_transform(df['synopsis'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "tree_reg.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred = tree_reg.predict(X_test)\n",
    "\n",
    "y_train_pred = tree_reg.predict(X_train)\n",
    "\n",
    "print(mean_squared_error(y_test, y_test_pred))\n",
    "print(mean_squared_error(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38b7ae3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9569137053974088\n",
      "0.002713765829728543\n"
     ]
    }
   ],
   "source": [
    "#https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "#with stop words & nmax_df 0.7 & gram_range = (1,2)\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv = CountVectorizer(lowercase=True,stop_words='english', max_df = 0.7, ngram_range = (1,3), tokenizer = token.tokenize)\n",
    "X= cv.fit_transform(df['synopsis'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "tree_reg.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred = tree_reg.predict(X_test)\n",
    "\n",
    "y_train_pred = tree_reg.predict(X_train)\n",
    "\n",
    "print(mean_squared_error(y_test, y_test_pred))\n",
    "print(mean_squared_error(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccdb56a",
   "metadata": {},
   "source": [
    "ngram of (1,3) has a slightly better score. However we will just stick with (1,2) since (1,3) takes too long to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe95e91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
