{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11f92f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "#https://www.nltk.org/\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import tensorflow as tf\n",
    "import ast\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cac52835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keybert import KeyBERT\n",
    "# kw_model = KeyBERT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f93e686d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>synopsis</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Haikyuu!! Second Season</td>\n",
       "      <td>Following their participation at the Inter-Hig...</td>\n",
       "      <td>8.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shigatsu wa Kimi no Uso</td>\n",
       "      <td>Music accompanies the path of the human metron...</td>\n",
       "      <td>8.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Made in Abyss</td>\n",
       "      <td>The Abyss—a gaping chasm stretching down into ...</td>\n",
       "      <td>8.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fullmetal Alchemist: Brotherhood</td>\n",
       "      <td>\"In order for something to be obtained, someth...</td>\n",
       "      <td>9.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kizumonogatari III: Reiketsu-hen</td>\n",
       "      <td>After helping revive the legendary vampire Kis...</td>\n",
       "      <td>8.83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              title  \\\n",
       "0           Haikyuu!! Second Season   \n",
       "1           Shigatsu wa Kimi no Uso   \n",
       "2                     Made in Abyss   \n",
       "3  Fullmetal Alchemist: Brotherhood   \n",
       "4  Kizumonogatari III: Reiketsu-hen   \n",
       "\n",
       "                                            synopsis  score  \n",
       "0  Following their participation at the Inter-Hig...   8.82  \n",
       "1  Music accompanies the path of the human metron...   8.83  \n",
       "2  The Abyss—a gaping chasm stretching down into ...   8.83  \n",
       "3  \"In order for something to be obtained, someth...   9.23  \n",
       "4  After helping revive the legendary vampire Kis...   8.83  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extract the dataset we are interested in, synopsis, name, and rating.\n",
    "df = pd.read_csv('DataSet/animes.csv')\n",
    "df = df[['title','synopsis','score']]\n",
    "df.dropna(inplace=True)\n",
    "df = df[~df.title.duplicated(keep='first')]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "353283b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15192\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(15192, 100)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Method one for generating X vectors: hash.\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "X = df['synopsis'].to_numpy()\n",
    "y = df['score'].to_numpy()\n",
    "y = np.rint(y)\n",
    "print(len(y))\n",
    "vectorizer = HashingVectorizer(n_features=100, norm = None, alternate_sign = False)\n",
    "hashed_X = vectorizer.fit_transform(X)\n",
    "#print(hashed_X)\n",
    "hashed_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca612f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building training sets and test sets.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(hashed_X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc976f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.3645936163211583\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "model = DecisionTreeClassifier(max_depth=100)\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "score = accuracy_score(y_test, pred)\n",
    "print(\"Score: \" + str(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64b2d82",
   "metadata": {},
   "source": [
    "Accuracy of about 35%, which is better than guessing randomly (10%), but not very good for practical predictions.\n",
    "\n",
    "Pros of this method: Easy to implement\n",
    "\n",
    "Cons: Not interpretable, I have no idea what type of synopsis correspond to the predictions made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "373cdc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "#https://www.nltk.org/book/ch06.html\n",
    "#https://www.datacamp.com/tutorial/text-analytics-beginners-nltk#sentiment-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83423df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "#tokenizer to remove unwanted elements from out data like symbols and numbers\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv = CountVectorizer(lowercase=True,stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize)\n",
    "text_counts= cv.fit_transform(df['synopsis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67b49d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://splunktool.com/unknown-label-type-error-when-sklearn-naive-bayes-used-with-floating-point-numbers\n",
    "score_int = df['score'].to_numpy()\n",
    "score_int = np.rint(score_int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_counts, score_int, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d6e4df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.48244844229925404\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "# Model Generation Using Multinomial Naive Bayes\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "predicted= clf.predict(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, predicted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70ed145",
   "metadata": {},
   "source": [
    "The accuracy is slightly higher with nltk implemented at around 47%, but there are still improvements to be made. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd8be29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
