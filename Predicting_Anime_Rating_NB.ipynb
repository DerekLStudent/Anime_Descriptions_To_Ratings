{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11f92f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "#https://www.nltk.org/\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#import tensorflow as tf\n",
    "import ast\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#Decision trees for regression\n",
    "#MSE for loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cac52835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keybert import KeyBERT\n",
    "# kw_model = KeyBERT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f93e686d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>synopsis</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Haikyuu!! Second Season</td>\n",
       "      <td>Following their participation at the Inter-Hig...</td>\n",
       "      <td>8.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shigatsu wa Kimi no Uso</td>\n",
       "      <td>Music accompanies the path of the human metron...</td>\n",
       "      <td>8.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Made in Abyss</td>\n",
       "      <td>The Abyss—a gaping chasm stretching down into ...</td>\n",
       "      <td>8.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fullmetal Alchemist: Brotherhood</td>\n",
       "      <td>\"In order for something to be obtained, someth...</td>\n",
       "      <td>9.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kizumonogatari III: Reiketsu-hen</td>\n",
       "      <td>After helping revive the legendary vampire Kis...</td>\n",
       "      <td>8.83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              title  \\\n",
       "0           Haikyuu!! Second Season   \n",
       "1           Shigatsu wa Kimi no Uso   \n",
       "2                     Made in Abyss   \n",
       "3  Fullmetal Alchemist: Brotherhood   \n",
       "4  Kizumonogatari III: Reiketsu-hen   \n",
       "\n",
       "                                            synopsis  score  \n",
       "0  Following their participation at the Inter-Hig...   8.82  \n",
       "1  Music accompanies the path of the human metron...   8.83  \n",
       "2  The Abyss—a gaping chasm stretching down into ...   8.83  \n",
       "3  \"In order for something to be obtained, someth...   9.23  \n",
       "4  After helping revive the legendary vampire Kis...   8.83  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extract the dataset we are interested in, synopsis, name, and rating.\n",
    "df = pd.read_csv('DataSet/animes.csv')\n",
    "df = df[['title','synopsis','score']]\n",
    "df.dropna(inplace=True)\n",
    "df = df[~df.title.duplicated(keep='first')]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "353283b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15192\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(15192, 100)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Method one for generating X vectors: hash.\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "X = df['synopsis'].to_numpy()\n",
    "y = df['score'].to_numpy()\n",
    "y = np.rint(y)\n",
    "print(len(y))\n",
    "vectorizer = HashingVectorizer(n_features=100, norm = None, alternate_sign = False)\n",
    "hashed_X = vectorizer.fit_transform(X)\n",
    "#print(hashed_X)\n",
    "hashed_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca612f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building training sets and test sets.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(hashed_X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc976f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.35538005923000987\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "model = DecisionTreeClassifier(max_depth=100)\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "score = accuracy_score(y_test, pred)\n",
    "print(\"Score: \" + str(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64b2d82",
   "metadata": {},
   "source": [
    "Accuracy of about 35%, which is better than guessing randomly (10%), but not very good for practical predictions.\n",
    "\n",
    "Pros of this method: Easy to implement\n",
    "\n",
    "Cons: Not interpretable, I have no idea what type of synopsis correspond to the predictions made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "373cdc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "#https://www.nltk.org/book/ch06.html\n",
    "#https://www.datacamp.com/tutorial/text-analytics-beginners-nltk#sentiment-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83423df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "#tokenizer to remove unwanted elements from out data like symbols and numbers\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv = CountVectorizer(lowercase=True,stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize)\n",
    "text_counts= cv.fit_transform(df['synopsis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67b49d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://splunktool.com/unknown-label-type-error-when-sklearn-naive-bayes-used-with-floating-point-numbers\n",
    "score_int = df['score'].to_numpy()\n",
    "score_int = np.rint(score_int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_counts, score_int, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d6e4df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.48244844229925404\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "# Model Generation Using Multinomial Naive Bayes\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "predicted= clf.predict(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, predicted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70ed145",
   "metadata": {},
   "source": [
    "The accuracy is slightly higher with nltk implemented at around 47%, but there are still improvements to be made. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3dd8be29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "#Using old tokenizer, subject to change.\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv = CountVectorizer(lowercase=True,stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize)\n",
    "X= cv.fit_transform(df['synopsis'])\n",
    "\n",
    "#Now that we are using decision tree regression, we can use the scores as floats.\n",
    "y = df['score'].to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "\n",
    "tree_reg.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred = tree_reg.predict(X_test)\n",
    "\n",
    "y_train_pred = tree_reg.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a075499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0057624047782288\n",
      "0.0027221966491129085\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "print(mean_squared_error(y_test, y_test_pred))\n",
    "print(mean_squared_error(y_train, y_train_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d277d4",
   "metadata": {},
   "source": [
    "It seems this initial run on regression tree overfit the data, it has a really low mean squared error on the training data, but seems to be relatively high on the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87ca3962",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11696/2528884717.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mtree_reg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0my_test_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtree_reg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Software\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1340\u001b[0m         \"\"\"\n\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1342\u001b[1;33m         super().fit(\n\u001b[0m\u001b[0;32m   1343\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1344\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Software\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    456\u001b[0m             )\n\u001b[0;32m    457\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 458\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    459\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "#with stop words & max_df 0.7 & ngram_range = (1,2)\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv = CountVectorizer(lowercase=True,stop_words='english', max_df = 0.7, ngram_range = (1,2), tokenizer = token.tokenize)\n",
    "X= cv.fit_transform(df['synopsis'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "tree_reg.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred = tree_reg.predict(X_test)\n",
    "\n",
    "y_train_pred = tree_reg.predict(X_train)\n",
    "\n",
    "print(mean_squared_error(y_test, y_test_pred))\n",
    "print(mean_squared_error(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2847afe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#without stop words & ngram_range = (1,1)\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv = CountVectorizer(lowercase=True, ngram_range = (1,1), tokenizer = token.tokenize)\n",
    "X= cv.fit_transform(df['synopsis'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "tree_reg.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred = tree_reg.predict(X_test)\n",
    "\n",
    "y_train_pred = tree_reg.predict(X_train)\n",
    "\n",
    "print(mean_squared_error(y_test, y_test_pred))\n",
    "print(mean_squared_error(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61716f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#without stop words & ngram_range = (1,2)\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv = CountVectorizer(lowercase=True, ngram_range = (1,2), tokenizer = token.tokenize)\n",
    "X= cv.fit_transform(df['synopsis'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "tree_reg.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred = tree_reg.predict(X_test)\n",
    "\n",
    "y_train_pred = tree_reg.predict(X_train)\n",
    "\n",
    "print(mean_squared_error(y_test, y_test_pred))\n",
    "print(mean_squared_error(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6f0209",
   "metadata": {},
   "source": [
    "The removal of stop words helps get a better score. Changing the ngram to unigrams and bigrams also helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2c1497",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with stop words & max_df 0.7 & ngram_range = (1,2)\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv = CountVectorizer(lowercase=True,stop_words='english', max_df = 0.7, ngram_range = (1,2), tokenizer = token.tokenize)\n",
    "X= cv.fit_transform(df['synopsis'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "tree_reg.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred = tree_reg.predict(X_test)\n",
    "\n",
    "y_train_pred = tree_reg.predict(X_train)\n",
    "\n",
    "print(mean_squared_error(y_test, y_test_pred))\n",
    "print(mean_squared_error(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b7ae3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "#with stop words & nmax_df 0.7 & gram_range = (1,2)\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv = CountVectorizer(lowercase=True,stop_words='english', max_df = 0.7, ngram_range = (1,3), tokenizer = token.tokenize)\n",
    "X= cv.fit_transform(df['synopsis'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "tree_reg.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred = tree_reg.predict(X_test)\n",
    "\n",
    "y_train_pred = tree_reg.predict(X_train)\n",
    "\n",
    "print(mean_squared_error(y_test, y_test_pred))\n",
    "print(mean_squared_error(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccdb56a",
   "metadata": {},
   "source": [
    "ngram of (1,3) has a slightly better score. However we will just stick with (1,2) since (1,3) takes too long to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe95e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with stop words & max_df 0.7 & ngram_range = (1,2)\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv = CountVectorizer(lowercase=True,stop_words='english', max_df = 0.7, ngram_range = (1,2), tokenizer = token.tokenize)\n",
    "X= cv.fit_transform(df['synopsis'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "tree_reg = DecisionTreeRegressor(max_depth=30)\n",
    "tree_reg.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred = tree_reg.predict(X_test)\n",
    "\n",
    "y_train_pred = tree_reg.predict(X_train)\n",
    "\n",
    "print(mean_squared_error(y_test, y_test_pred))\n",
    "print(mean_squared_error(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0053bd",
   "metadata": {},
   "source": [
    "Limiting Tree depth significantly improves accuracy on the test data, let's try to find optimal depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b1ebd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = []\n",
    "training_MSE= []\n",
    "test_MSE = []\n",
    "for i in range(30):\n",
    "    if(i == 0):\n",
    "        pass\n",
    "    else:\n",
    "        #with stop words & max_df 0.7 & ngram_range = (1,2)\n",
    "        token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "        cv = CountVectorizer(lowercase=True,stop_words='english', max_df = 0.7, ngram_range = (1,2), tokenizer = token.tokenize)\n",
    "        X= cv.fit_transform(df['synopsis'])\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "        tree_reg = DecisionTreeRegressor(max_depth=i)\n",
    "        tree_reg.fit(X_train, y_train)\n",
    "\n",
    "        y_test_pred = tree_reg.predict(X_test)\n",
    "\n",
    "        y_train_pred = tree_reg.predict(X_train)\n",
    "        \n",
    "        training_MSE.append(mean_squared_error(y_train, y_train_pred))\n",
    "        test_MSE.append(mean_squared_error(y_test, y_test_pred))\n",
    "        depth.append(i)\n",
    "plt.plot(depth, training_MSE, label = \"training MSE\")\n",
    "plt.plot(depth, test_MSE, label = \"test MSE\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0d763fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>synopsis</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fullmetal Alchemist: Brotherhood</td>\n",
       "      <td>\"In order for something to be obtained, someth...</td>\n",
       "      <td>9.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>Koe no Katachi</td>\n",
       "      <td>As a wild youth, elementary school student Sho...</td>\n",
       "      <td>9.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>3-gatsu no Lion 2nd Season</td>\n",
       "      <td>Now in his second year of high school, Rei Kir...</td>\n",
       "      <td>9.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>Ginga Eiyuu Densetsu</td>\n",
       "      <td>The 150-year-long stalemate between the two in...</td>\n",
       "      <td>9.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768</th>\n",
       "      <td>Gintama'</td>\n",
       "      <td>After a one-year hiatus, Shinpachi Shimura ret...</td>\n",
       "      <td>9.04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                title  \\\n",
       "3    Fullmetal Alchemist: Brotherhood   \n",
       "765                    Koe no Katachi   \n",
       "766        3-gatsu no Lion 2nd Season   \n",
       "767              Ginga Eiyuu Densetsu   \n",
       "768                          Gintama'   \n",
       "\n",
       "                                              synopsis  score  \n",
       "3    \"In order for something to be obtained, someth...   9.23  \n",
       "765  As a wild youth, elementary school student Sho...   9.01  \n",
       "766  Now in his second year of high school, Rei Kir...   9.02  \n",
       "767  The 150-year-long stalemate between the two in...   9.03  \n",
       "768  After a one-year hiatus, Shinpachi Shimura ret...   9.04  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://towardsdatascience.com/natural-language-processing-count-vectorization-with-scikit-learn-e7804269bb5e\n",
    "#https://kavita-ganesan.com/how-to-use-countvectorizer/#.Y4p3VXbMKUk\n",
    "#https://investigate.ai/text-analysis/counting-words-with-scikit-learns-countvectorizer/\n",
    "#https://towardsdatascience.com/basics-of-countvectorizer-e26677900f9c\n",
    "df_high = df.loc[df['score'] >= 9]\n",
    "df_high.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1dc174c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'order': 399,\n",
       " 'obtained': 392,\n",
       " 'equal': 181,\n",
       " 'value': 632,\n",
       " 'lost': 336,\n",
       " 'alchemy': 24,\n",
       " 'bound': 70,\n",
       " 'law': 315,\n",
       " 'equivalent': 182,\n",
       " 'exchange': 187,\n",
       " 'young': 658,\n",
       " 'brothers': 75,\n",
       " 'edward': 167,\n",
       " 'alphonse': 31,\n",
       " 'elric': 169,\n",
       " 'realize': 460,\n",
       " 'attempting': 48,\n",
       " 'human': 271,\n",
       " 'transmutation': 616,\n",
       " 'forbidden': 209,\n",
       " 'act': 7,\n",
       " 'pay': 414,\n",
       " 'terrible': 594,\n",
       " 'price': 441,\n",
       " 'transgression': 615,\n",
       " 'loses': 335,\n",
       " 'left': 320,\n",
       " 'leg': 321,\n",
       " 'physical': 424,\n",
       " 'body': 67,\n",
       " 'desperate': 143,\n",
       " 'sacrifice': 496,\n",
       " 'right': 490,\n",
       " 'arm': 42,\n",
       " 'able': 3,\n",
       " 'affix': 14,\n",
       " 'soul': 548,\n",
       " 'suit': 576,\n",
       " 'armor': 44,\n",
       " 'devastated': 146,\n",
       " 'hope': 268,\n",
       " 'eventually': 184,\n",
       " 'return': 484,\n",
       " 'original': 401,\n",
       " 'bodies': 66,\n",
       " 'gives': 235,\n",
       " 'inspiration': 286,\n",
       " 'obtain': 391,\n",
       " 'metal': 356,\n",
       " 'limbs': 326,\n",
       " 'called': 82,\n",
       " 'automail': 51,\n",
       " 'state': 561,\n",
       " 'alchemist': 23,\n",
       " 'fullmetal': 218,\n",
       " 'years': 656,\n",
       " 'searching': 505,\n",
       " 'later': 314,\n",
       " 'seek': 509,\n",
       " 'philosopher': 422,\n",
       " 'stone': 563,\n",
       " 'mythical': 377,\n",
       " 'relic': 473,\n",
       " 'allows': 28,\n",
       " 'overcome': 406,\n",
       " 'military': 359,\n",
       " 'allies': 27,\n",
       " 'colonel': 107,\n",
       " 'roy': 494,\n",
       " 'mustang': 375,\n",
       " 'lieutenant': 323,\n",
       " 'riza': 492,\n",
       " 'hawkeye': 249,\n",
       " 'maes': 341,\n",
       " 'hughes': 270,\n",
       " 'caught': 88,\n",
       " 'nationwide': 379,\n",
       " 'conspiracy': 115,\n",
       " 'leads': 317,\n",
       " 'true': 618,\n",
       " 'nature': 380,\n",
       " 'elusive': 170,\n",
       " 'country': 122,\n",
       " 'murky': 374,\n",
       " 'history': 263,\n",
       " 'finding': 202,\n",
       " 'serial': 513,\n",
       " 'killer': 304,\n",
       " 'racing': 455,\n",
       " 'time': 601,\n",
       " 'ask': 45,\n",
       " 'doing': 159,\n",
       " 'make': 342,\n",
       " 'away': 53,\n",
       " 'humanity': 272,\n",
       " 'wild': 648,\n",
       " 'youth': 659,\n",
       " 'elementary': 168,\n",
       " 'school': 501,\n",
       " 'student': 571,\n",
       " 'shouya': 531,\n",
       " 'ishida': 290,\n",
       " 'sought': 547,\n",
       " 'beat': 57,\n",
       " 'boredom': 69,\n",
       " 'cruelest': 131,\n",
       " 'ways': 645,\n",
       " 'deaf': 136,\n",
       " 'shouko': 529,\n",
       " 'nishimiya': 385,\n",
       " 'transfers': 613,\n",
       " 'class': 101,\n",
       " 'rest': 481,\n",
       " 'thoughtlessly': 597,\n",
       " 'bully': 77,\n",
       " 'fun': 219,\n",
       " 'mother': 372,\n",
       " 'notifies': 388,\n",
       " 'singled': 536,\n",
       " 'blamed': 64,\n",
       " 'transferring': 612,\n",
       " 'mercy': 355,\n",
       " 'classmates': 102,\n",
       " 'heartlessly': 253,\n",
       " 'ostracized': 402,\n",
       " 'middle': 358,\n",
       " 'teachers': 591,\n",
       " 'turn': 621,\n",
       " 'blind': 65,\n",
       " 'eye': 190,\n",
       " 'year': 654,\n",
       " 'high': 257,\n",
       " 'plagued': 427,\n",
       " 'wrongdoings': 651,\n",
       " 'boy': 71,\n",
       " 'sincerely': 535,\n",
       " 'regretting': 469,\n",
       " 'past': 412,\n",
       " 'actions': 9,\n",
       " 'sets': 515,\n",
       " 'journey': 296,\n",
       " 'redemption': 467,\n",
       " 'meet': 351,\n",
       " 'amends': 34,\n",
       " 'koe': 309,\n",
       " 'katachi': 301,\n",
       " 'tells': 593,\n",
       " 'heartwarming': 254,\n",
       " 'tale': 588,\n",
       " 'reunion': 487,\n",
       " 'honest': 267,\n",
       " 'attempts': 49,\n",
       " 'redeem': 466,\n",
       " 'continually': 116,\n",
       " 'haunted': 247,\n",
       " 'shadows': 516,\n",
       " 'second': 507,\n",
       " 'rei': 470,\n",
       " 'kiriyama': 308,\n",
       " 'continues': 117,\n",
       " 'pushing': 453,\n",
       " 'struggles': 570,\n",
       " 'professional': 445,\n",
       " 'shogi': 528,\n",
       " 'world': 650,\n",
       " 'personal': 417,\n",
       " 'life': 324,\n",
       " 'surrounded': 581,\n",
       " 'vibrant': 634,\n",
       " 'personalities': 418,\n",
       " 'hall': 242,\n",
       " 'club': 106,\n",
       " 'local': 331,\n",
       " 'community': 110,\n",
       " 'solitary': 543,\n",
       " 'shell': 519,\n",
       " 'slowly': 540,\n",
       " 'begins': 60,\n",
       " 'crack': 124,\n",
       " 'kawamoto': 302,\n",
       " 'sisters': 538,\n",
       " 'akari': 20,\n",
       " 'hinata': 261,\n",
       " 'momo': 368,\n",
       " 'forge': 211,\n",
       " 'affectionate': 13,\n",
       " 'familial': 193,\n",
       " 'bond': 68,\n",
       " 'ties': 600,\n",
       " 'realizes': 461,\n",
       " 'burdened': 78,\n",
       " 'emotional': 175,\n",
       " 'hardships': 245,\n",
       " 'learning': 318,\n",
       " 'rely': 474,\n",
       " 'supporting': 579,\n",
       " 'nonetheless': 387,\n",
       " 'easy': 165,\n",
       " 'tournaments': 607,\n",
       " 'championships': 90,\n",
       " 'title': 604,\n",
       " 'matches': 348,\n",
       " 'pressure': 440,\n",
       " 'mounts': 373,\n",
       " 'advances': 11,\n",
       " 'ranks': 456,\n",
       " 'encounters': 178,\n",
       " 'incredibly': 283,\n",
       " 'skilled': 539,\n",
       " 'opponents': 397,\n",
       " 'manages': 343,\n",
       " 'relationships': 472,\n",
       " 'grown': 240,\n",
       " 'close': 105,\n",
       " 'player': 429,\n",
       " 'search': 504,\n",
       " 'reason': 464,\n",
       " 'plays': 431,\n",
       " 'game': 225,\n",
       " 'defines': 138,\n",
       " 'career': 84,\n",
       " '150': 0,\n",
       " 'long': 334,\n",
       " 'stalemate': 554,\n",
       " 'interstellar': 287,\n",
       " 'superpowers': 577,\n",
       " 'galactic': 223,\n",
       " 'empire': 176,\n",
       " 'free': 214,\n",
       " 'planets': 428,\n",
       " 'alliance': 26,\n",
       " 'comes': 109,\n",
       " 'end': 179,\n",
       " 'new': 382,\n",
       " 'generation': 228,\n",
       " 'leaders': 316,\n",
       " 'arises': 40,\n",
       " 'idealistic': 277,\n",
       " 'genius': 229,\n",
       " 'reinhard': 471,\n",
       " 'von': 638,\n",
       " 'lohengramm': 333,\n",
       " 'fpa': 213,\n",
       " 'reserved': 480,\n",
       " 'historian': 262,\n",
       " 'yang': 652,\n",
       " 'wenli': 647,\n",
       " 'climbs': 103,\n",
       " 'aid': 18,\n",
       " 'childhood': 97,\n",
       " 'friend': 216,\n",
       " 'siegfried': 532,\n",
       " 'kircheis': 307,\n",
       " 'fight': 200,\n",
       " 'war': 642,\n",
       " 'remnants': 477,\n",
       " 'crumbling': 132,\n",
       " 'goldenbaum': 237,\n",
       " 'dynasty': 164,\n",
       " 'sister': 537,\n",
       " 'kaiser': 300,\n",
       " 'unify': 628,\n",
       " 'genuine': 230,\n",
       " 'ruler': 495,\n",
       " 'galaxy': 224,\n",
       " 'strong': 569,\n",
       " 'supporter': 578,\n",
       " 'democratic': 139,\n",
       " 'ideals': 278,\n",
       " 'stand': 556,\n",
       " 'firm': 204,\n",
       " 'beliefs': 61,\n",
       " 'despite': 144,\n",
       " 'pupil': 451,\n",
       " 'julian': 298,\n",
       " 'mintz': 360,\n",
       " 'autocracy': 50,\n",
       " 'solution': 544,\n",
       " 'ideologies': 279,\n",
       " 'clash': 100,\n",
       " 'amidst': 35,\n",
       " 'casualties': 87,\n",
       " 'strategic': 566,\n",
       " 'masterminds': 347,\n",
       " 'real': 458,\n",
       " 'battle': 56,\n",
       " 'hiatus': 256,\n",
       " 'shinpachi': 524,\n",
       " 'shimura': 522,\n",
       " 'returns': 486,\n",
       " 'edo': 166,\n",
       " 'stumble': 572,\n",
       " 'shocking': 527,\n",
       " 'surprise': 580,\n",
       " 'gintoki': 233,\n",
       " 'kagura': 299,\n",
       " 'fellow': 199,\n",
       " 'yorozuya': 657,\n",
       " 'members': 353,\n",
       " 'completely': 111,\n",
       " 'different': 148,\n",
       " 'characters': 93,\n",
       " 'fleeing': 206,\n",
       " 'headquarters': 251,\n",
       " 'confusion': 113,\n",
       " 'finds': 203,\n",
       " 'denizens': 140,\n",
       " 'undergone': 626,\n",
       " 'impossibly': 282,\n",
       " 'extreme': 189,\n",
       " 'changes': 92,\n",
       " 'appearance': 38,\n",
       " 'personality': 419,\n",
       " 'unbelievably': 622,\n",
       " 'otae': 403,\n",
       " 'married': 346,\n",
       " 'shinsengumi': 525,\n",
       " 'chief': 95,\n",
       " 'shameless': 517,\n",
       " 'stalker': 555,\n",
       " 'isao': 289,\n",
       " 'kondou': 310,\n",
       " 'pregnant': 438,\n",
       " 'child': 96,\n",
       " 'bewildered': 63,\n",
       " 'agrees': 17,\n",
       " 'join': 294,\n",
       " 'request': 479,\n",
       " 'startling': 560,\n",
       " 'transformations': 614,\n",
       " 'afoot': 15,\n",
       " 'organization': 400,\n",
       " 'discovering': 151,\n",
       " 'vice': 635,\n",
       " 'toushirou': 608,\n",
       " 'hijikata': 259,\n",
       " 'remained': 475,\n",
       " 'unchanged': 623,\n",
       " 'unlikely': 629,\n",
       " 'ally': 29,\n",
       " 'set': 514,\n",
       " 'city': 99,\n",
       " 'remember': 476,\n",
       " 'dirty': 150,\n",
       " 'jokes': 295,\n",
       " 'tongue': 606,\n",
       " 'cheek': 94,\n",
       " 'parodies': 410,\n",
       " 'references': 468,\n",
       " 'gintama': 232,\n",
       " 'follows': 208,\n",
       " 'team': 592,\n",
       " 'misadventures': 362,\n",
       " 'alien': 25,\n",
       " 'filled': 201,\n",
       " 'loving': 337,\n",
       " 'broke': 74,\n",
       " 'living': 330,\n",
       " 'alternate': 33,\n",
       " 'reality': 459,\n",
       " 'swords': 584,\n",
       " 'prohibited': 447,\n",
       " 'overlords': 407,\n",
       " 'conquered': 114,\n",
       " 'japan': 292,\n",
       " 'try': 620,\n",
       " 'thrive': 599,\n",
       " 'work': 649,\n",
       " 'hands': 243,\n",
       " 'haven': 248,\n",
       " 'paid': 409,\n",
       " 'does': 157,\n",
       " 'gin': 231,\n",
       " 'chan': 91,\n",
       " 'really': 462,\n",
       " 'spend': 550,\n",
       " 'cash': 86,\n",
       " 'playing': 430,\n",
       " 'pachinko': 408,\n",
       " 'drunkenly': 162,\n",
       " 'staggers': 553,\n",
       " 'home': 266,\n",
       " 'night': 384,\n",
       " 'spaceship': 549,\n",
       " 'crashes': 125,\n",
       " 'nearby': 381,\n",
       " 'fatally': 196,\n",
       " 'injured': 285,\n",
       " 'crew': 127,\n",
       " 'member': 352,\n",
       " 'emerges': 173,\n",
       " 'ship': 526,\n",
       " 'strange': 564,\n",
       " 'clock': 104,\n",
       " 'shaped': 518,\n",
       " 'device': 147,\n",
       " 'warning': 643,\n",
       " 'powerful': 436,\n",
       " 'safeguarded': 498,\n",
       " 'mistaking': 364,\n",
       " 'alarm': 22,\n",
       " 'proceeds': 443,\n",
       " 'smash': 541,\n",
       " 'morning': 370,\n",
       " 'suddenly': 575,\n",
       " 'discovers': 152,\n",
       " 'outside': 405,\n",
       " 'apartment': 37,\n",
       " 'come': 108,\n",
       " 'standstill': 558,\n",
       " 'fixed': 205,\n",
       " 'usual': 631,\n",
       " 'simple': 534,\n",
       " 'humor': 274,\n",
       " 'moments': 367,\n",
       " 'heartfelt': 252,\n",
       " 'emotion': 174,\n",
       " 'fourth': 212,\n",
       " 'season': 506,\n",
       " 'friends': 217,\n",
       " 'facing': 192,\n",
       " 'hilarious': 260,\n",
       " 'dangerous': 133,\n",
       " 'crises': 129,\n",
       " 'seeking': 510,\n",
       " 'restore': 482,\n",
       " 'diminishing': 149,\n",
       " 'survey': 582,\n",
       " 'corps': 120,\n",
       " 'embark': 172,\n",
       " 'mission': 363,\n",
       " 'retake': 483,\n",
       " 'wall': 641,\n",
       " 'maria': 345,\n",
       " 'merciless': 354,\n",
       " 'titans': 603,\n",
       " 'takes': 586,\n",
       " 'stage': 552,\n",
       " 'returning': 485,\n",
       " 'tattered': 590,\n",
       " 'shiganshina': 520,\n",
       " 'district': 154,\n",
       " 'eren': 183,\n",
       " 'yeager': 653,\n",
       " 'town': 609,\n",
       " 'oddly': 393,\n",
       " 'unoccupied': 630,\n",
       " 'outer': 404,\n",
       " 'gate': 226,\n",
       " 'plugged': 433,\n",
       " 'strangely': 565,\n",
       " 'encounter': 177,\n",
       " 'opposition': 398,\n",
       " 'progresses': 446,\n",
       " 'smoothly': 542,\n",
       " 'armin': 43,\n",
       " 'arlert': 41,\n",
       " 'highly': 258,\n",
       " 'suspicious': 583,\n",
       " 'enemy': 180,\n",
       " 'absence': 4,\n",
       " 'distressing': 153,\n",
       " 'signs': 533,\n",
       " 'potential': 435,\n",
       " 'scheme': 500,\n",
       " 'shingeki': 523,\n",
       " 'kyojin': 312,\n",
       " 'vows': 639,\n",
       " 'alongside': 30,\n",
       " 'strive': 568,\n",
       " 'countless': 121,\n",
       " 'sacrifices': 497,\n",
       " 'carve': 85,\n",
       " 'path': 413,\n",
       " 'victory': 636,\n",
       " 'uncover': 625,\n",
       " 'secrets': 508,\n",
       " 'locked': 332,\n",
       " 'family': 194,\n",
       " 'basement': 55,\n",
       " 'mitsuha': 365,\n",
       " 'miyamizu': 366,\n",
       " 'girl': 234,\n",
       " 'yearns': 655,\n",
       " 'live': 327,\n",
       " 'bustling': 80,\n",
       " 'tokyo': 605,\n",
       " 'dream': 161,\n",
       " 'stands': 557,\n",
       " 'stark': 559,\n",
       " 'contrast': 119,\n",
       " 'present': 439,\n",
       " 'countryside': 123,\n",
       " 'taki': 587,\n",
       " 'tachibana': 585,\n",
       " 'lives': 329,\n",
       " 'busy': 81,\n",
       " 'juggling': 297,\n",
       " 'job': 293,\n",
       " 'hopes': 269,\n",
       " 'future': 221,\n",
       " 'architecture': 39,\n",
       " 'day': 135,\n",
       " 'awakens': 52,\n",
       " 'room': 493,\n",
       " 'humble': 273,\n",
       " 'pursuit': 452,\n",
       " 'answer': 36,\n",
       " 'phenomenon': 421,\n",
       " 'begin': 59,\n",
       " 'kimi': 306,\n",
       " 'na': 378,\n",
       " 'wa': 640,\n",
       " 'revolves': 488,\n",
       " 'dramatic': 160,\n",
       " 'impact': 281,\n",
       " 'weaving': 646,\n",
       " 'fabric': 191,\n",
       " 'held': 255,\n",
       " 'fate': 197,\n",
       " 'circumstance': 98,\n",
       " 'hunter': 275,\n",
       " 'hunters': 276,\n",
       " 'exist': 188,\n",
       " 'perform': 415,\n",
       " 'manner': 344,\n",
       " 'tasks': 589,\n",
       " 'like': 325,\n",
       " 'capturing': 83,\n",
       " 'criminals': 128,\n",
       " 'bravely': 72,\n",
       " 'treasures': 617,\n",
       " 'uncharted': 624,\n",
       " 'territories': 595,\n",
       " 'old': 396,\n",
       " 'gon': 238,\n",
       " 'freecss': 215,\n",
       " 'determined': 145,\n",
       " 'best': 62,\n",
       " 'possible': 434,\n",
       " 'father': 198,\n",
       " 'ago': 16,\n",
       " 'abandoned': 2,\n",
       " 'son': 545,\n",
       " 'soon': 546,\n",
       " 'achieving': 6,\n",
       " 'goals': 236,\n",
       " 'far': 195,\n",
       " 'challenging': 89,\n",
       " 'imagined': 280,\n",
       " 'way': 644,\n",
       " 'official': 394,\n",
       " 'befriends': 58,\n",
       " 'lively': 328,\n",
       " 'doctor': 156,\n",
       " 'training': 611,\n",
       " 'leorio': 322,\n",
       " 'vengeful': 633,\n",
       " 'kurapika': 311,\n",
       " 'rebellious': 465,\n",
       " 'ex': 185,\n",
       " 'assassin': 46,\n",
       " 'killua': 305,\n",
       " 'attain': 47,\n",
       " 'desires': 142,\n",
       " 'exam': 186,\n",
       " 'notorious': 389,\n",
       " 'low': 338,\n",
       " 'success': 574,\n",
       " 'rate': 457,\n",
       " 'probability': 442,\n",
       " 'death': 137,\n",
       " 'adventure': 12,\n",
       " 'puts': 454,\n",
       " 'plethora': 432,\n",
       " 'monsters': 369,\n",
       " 'creatures': 126,\n",
       " 'truly': 619,\n",
       " 'means': 350,\n",
       " 'self': 511,\n",
       " 'proclaimed': 444,\n",
       " 'mad': 340,\n",
       " 'scientist': 503,\n",
       " 'rintarou': 491,\n",
       " 'okabe': 395,\n",
       " 'rents': 478,\n",
       " 'rickety': 489,\n",
       " 'building': 76,\n",
       " 'akihabara': 21,\n",
       " 'indulges': 284,\n",
       " 'hobby': 264,\n",
       " 'inventing': 288,\n",
       " 'prospective': 449,\n",
       " 'gadgets': 222,\n",
       " 'lab': 313,\n",
       " 'mayuri': 349,\n",
       " 'shiina': 521,\n",
       " 'air': 19,\n",
       " 'headed': 250,\n",
       " 'hashida': 246,\n",
       " 'itaru': 291,\n",
       " 'perverted': 420,\n",
       " 'hacker': 241,\n",
       " 'nicknamed': 383,\n",
       " 'daru': 134,\n",
       " 'pass': 411,\n",
       " 'tinkering': 602,\n",
       " 'promising': 448,\n",
       " 'contraption': 118,\n",
       " 'machine': 339,\n",
       " 'dubbed': 163,\n",
       " 'phone': 423,\n",
       " 'microwave': 357,\n",
       " 'performs': 416,\n",
       " 'function': 220,\n",
       " 'morphing': 371,\n",
       " 'bananas': 54,\n",
       " 'piles': 425,\n",
       " 'green': 239,\n",
       " 'gel': 227,\n",
       " 'miraculous': 361,\n",
       " 'doesn': 158,\n",
       " 'provide': 450,\n",
       " 'concrete': 112,\n",
       " 'scientific': 502,\n",
       " 'breakthrough': 73,\n",
       " 'spurred': 551,\n",
       " 'action': 8,\n",
       " 'string': 567,\n",
       " 'mysterious': 376,\n",
       " 'happenings': 244,\n",
       " 'stumbling': 573,\n",
       " 'unexpected': 627,\n",
       " 'send': 512,\n",
       " 'emails': 171,\n",
       " 'altering': 32,\n",
       " 'flow': 207,\n",
       " 'adapted': 10,\n",
       " 'critically': 130,\n",
       " 'acclaimed': 5,\n",
       " 'visual': 637,\n",
       " 'novel': 390,\n",
       " '5pb': 1,\n",
       " 'nitroplus': 386,\n",
       " 'steins': 562,\n",
       " 'depths': 141,\n",
       " 'theory': 596,\n",
       " 'practicality': 437,\n",
       " 'forced': 210,\n",
       " 'diverging': 155,\n",
       " 'threads': 598,\n",
       " 'shoulder': 530,\n",
       " 'burdens': 79,\n",
       " 'holding': 265,\n",
       " 'key': 303,\n",
       " 'realm': 463,\n",
       " 'pinocchio': 426,\n",
       " 'learns': 319,\n",
       " 'traffic': 610,\n",
       " 'safety': 499}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(lowercase=True, stop_words='english', max_df = 0.7)\n",
    "X= cv.fit_transform(df_high['synopsis'])\n",
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da84043b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    150  5pb  abandoned  able  absence  acclaimed  achieving  act  action  \\\n",
      "0     0    0          0     1        0          0          0    1       0   \n",
      "1     0    0          0     0        0          0          0    0       0   \n",
      "2     0    0          0     0        0          0          0    0       0   \n",
      "3     1    0          0     0        0          0          0    0       0   \n",
      "4     0    0          0     0        0          0          0    0       0   \n",
      "5     0    0          0     0        0          0          0    0       0   \n",
      "6     0    0          0     0        1          0          0    0       0   \n",
      "7     0    0          0     0        0          0          0    0       0   \n",
      "8     0    0          1     0        0          0          1    0       0   \n",
      "9     0    1          0     0        0          1          0    0       1   \n",
      "10    0    0          0     0        0          0          0    0       0   \n",
      "\n",
      "    actions  ...  world  wrongdoings  yang  yeager  year  yearns  years  \\\n",
      "0         0  ...      0            0     0       0     0       0      1   \n",
      "1         1  ...      0            1     0       0     1       0      0   \n",
      "2         0  ...      1            0     0       0     1       0      0   \n",
      "3         0  ...      0            0     2       0     1       0      0   \n",
      "4         0  ...      1            0     0       0     1       0      0   \n",
      "5         0  ...      1            0     0       0     0       0      0   \n",
      "6         0  ...      0            0     0       2     0       0      0   \n",
      "7         1  ...      0            0     0       0     0       1      0   \n",
      "8         0  ...      1            0     0       0     1       0      0   \n",
      "9         0  ...      0            0     0       0     0       0      0   \n",
      "10        0  ...      0            0     0       0     0       0      0   \n",
      "\n",
      "    yorozuya  young  youth  \n",
      "0          0      1      0  \n",
      "1          0      1      1  \n",
      "2          0      0      0  \n",
      "3          0      0      0  \n",
      "4          3      0      0  \n",
      "5          2      0      0  \n",
      "6          0      0      0  \n",
      "7          0      0      0  \n",
      "8          0      1      0  \n",
      "9          0      0      0  \n",
      "10         0      0      0  \n",
      "\n",
      "[11 rows x 660 columns]\n"
     ]
    }
   ],
   "source": [
    "high_arr = X.toarray()\n",
    "df = pd.DataFrame(data=high_arr ,columns = cv.get_feature_names_out())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befda4ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
